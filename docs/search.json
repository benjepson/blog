[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "First post!\nEverything I’ve done in the past has been in private industry, it’s time to step out and put some work in public.\nI’ve worked as an industrial food scientist and statistician.\nI’ll sometimes talk about how things work, probably more often how they don’t work.\nI’ll be adding videos on YouTube, check those out too."
  },
  {
    "objectID": "posts/Airqualityanalysis/index.html",
    "href": "posts/Airqualityanalysis/index.html",
    "title": "Indoor Air Quality Analysis",
    "section": "",
    "text": "I’m running an Airthings Wave Plus air quality meter in my basement.\nI got it awhile ago because the price was right, and I figured it would make for some interesting data projects!\nIt logs radon every hour, and every 5 minutes it logs: carbon dioxide, VOCs, humidity, temperature, and pressure. Airthings has a great phone app (bluetooth) and excellent web dashboards. For this project, I’ll be downloading the logs and working with them in R. They have an API if you want to get real time data, I plan to run a project with that in the future, but for now we’ll work with a fixed dataset.\nRadon is important to me - I live in an area where it can be a problem. Temperature isn’t too critical but since it’s logged anyways we’ll look at it, might also be an important predictor for other measurements. Humidity in the basement is important to know, I want to make sure it doesn’t get above 60% RH, at least not for long. We don’t want to have mold issues. I don’t know that it’s necessary to measure humidity, I believe I can feel when it’s too high, but since we are logging it will be nice information to have, to confirm my sensory experience. Pressure might be used as a predictor for forecasting. VOC is important to look at, I want to have healthy air. \\(CO_2\\) is (to me) a proxy for “freshness”, I doubt it will be at a harmful level, but elevated levels might indicate a need for ventilation.\nI assume through this project that the Wave Plus is accurate for each measurement. I don’t have secondary measurements for any of these to verify against. For what it’s worth I did have an inexpensive temperature/humidity meter running at the same time for awhile, but the battery ran out and I haven’t replaced it. The readings appeared very similar to the Wave Plus.\n\n\nCode\n### read in dataset\n#it comes in a single column, separated by \";\"\n# wave_data &lt;- read_delim(\"airthings_export_110623.csv\",\n#                           delim = \";\",\n#                           escape_double = FALSE,\n#                           col_types = cols(recorded = col_character()),\n#                           trim_ws = TRUE)\n# ### need to cleanup date column, there is a \"T\" between date & time\n# ### and turn it into date format\n# wave_data &lt;- wave_data %&gt;%\n#     mutate(recorded = as_datetime(gsub(pattern = \"T\",\n#                               replacement = \" \",\n#                               x = wave_data$recorded))\n#     ) %&gt;%\n# ### rename columns for convenience\n#     rename(date_time   = recorded,\n#            radon       = `RADON_SHORT_TERM_AVG pCi/L`,\n#            temperature = `TEMP °F`,\n#            humidity    = `HUMIDITY %`,\n#            pressure    = `PRESSURE mBar`,\n#            CO2         = `CO2 ppm`,\n#            VOC         = `VOC ppb`)\n# \n# ### remove first week, calibration period\n# ### you know close enough, let's just start april 1st\n# wave_data &lt;- wave_data %&gt;%\n#     filter(date_time &gt;= \"2023-04-01 00:00:00\")\n# \n# saveRDS(wave_data, file = \"wave_data.RDS\")\n\n\n\n\nCode\n#load the processed file\nwave_data &lt;- readRDS(\"wave_data.RDS\")\n\n\n\n\nFirst I will look at relative humidity. Humidity is a potential problem in the warmer months.\n\n\nCode\np1 &lt;- wave_data %&gt;%\n    select(date_time, humidity) %&gt;%\n    ggplot(aes(x=date_time, y=humidity))+\n    #geom_point()+\n    geom_line()+\n    labs(x = \"\", y = \"\", title = \"%RH April - November, average about 59\")+\n    geom_hline(yintercept = mean(wave_data$humidity), color = \"blue\")+\n    geom_hline(yintercept = 60, color = \"red\")\n\np2 &lt;- wave_data %&gt;%\n    select(date_time, humidity) %&gt;%\n    ggplot(aes(x=humidity))+\n    geom_histogram(color = \"white\", fill = \"light blue\") +\n    theme(axis.line = element_blank(),\n                                axis.text = element_blank(),\n                                axis.ticks = element_blank(),\n                                axis.title = element_blank()) + \n    coord_flip()\n\np1 + p2 + plot_layout(widths = c(5,1))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nLooking over this period, I can see in general a lot of the readings are below 60, with some above 60, and a few rare points above 65. It looks like RH is higher in May, and October - this might line up with the fact that the air conditioning isn’t running around those times. Makes a lot of sense that way. It’s hard to see each day, let’s look at a smaller time period to check for patterns:\n\n\nCode\nwave_data[30000:32000, ] %&gt;%\n    select(date_time, humidity) %&gt;%\n    ggplot(aes(x=date_time, y=humidity))+\n    #geom_point()+\n    geom_line()\n\n\n\n\n\n\n\n\n\nI didn’t mention yet, %RH is recorded in 0.5% intervals, so there is some chunkiness. I believe measuring every 5 minutes is excessive, I could probably measure once an hour (or less) or take a daily average. Humidity rises and falls but I don’t know how predictable that’s going to be. Definitely cyclic and it looks like it rises through the day, and falls during the night. Not surprising.\nInteractive plot for exploration:\n\n\nCode\nwave_data %&gt;%\n    select(date_time, humidity) %&gt;%\n    ggplot(aes(x=date_time, y=humidity))+\n    geom_line()+\n    labs(x = \"\", y = \"\", title = \"%RH April - November, average about 59\")+\n    geom_hline(yintercept = mean(wave_data$humidity), color = \"blue\")+\n    geom_hline(yintercept = 60, color = \"red\") -&gt; p \n\n    ggplotly(p, dynamicTicks = TRUE) %&gt;% \n        rangeslider()\n\n\n\n\n\n\nZooming in on sections helps me see the cyclic nature, in general humidity does rise during the day and fall at night.\n\n\nI’ve already learned enough to answer my question!\nThe basement felt fine, no issues! No need for a dehumidifier. Unless/until something changes - next year? Never?\nIn general, %RH stayed in a good range, there were some higher days but this didn’t seem to cause problems. Next I’ll look at how long the humidity was above 60%, even though it went higher from time to time I bet it was for relatively short intervals.\nKnowing how long humidity was above 60% will help me understand my conclusion that %RH was not an issue this year.\n\n\n\n\n\n\nCode\nhumid_RL &lt;- wave_data %&gt;% \n    select(date_time, humidity) %&gt;% \n    mutate(over_60 = case_when(humidity &gt; 60 ~ TRUE, \n                               .default = FALSE))\nsetDT(humid_RL)\nhumid_RL$RLID &lt;- rleid(humid_RL$over_60) #run length for gt/lt 60\n\nhumid_RL[over_60 == TRUE] %&gt;% \n    group_by(RLID) %&gt;% \n    summarize(hours_above_60 = (sum(over_60)) * 5 / 60) %&gt;% \n\n    ggplot(aes(x=RLID, y=hours_above_60))+\n    geom_col()+\n    labs(x = \"Run ID\", y = \"hours above 60% RH\", title = \"How long was each period of time above 60%RH?\")\n\n\n\n\n\n\n\n\n\nThis is neat - I see that most excursions above 60% RH were for about a day or less, a few were between 2-4 days, and the longest was about 6 days. Seems that while the humidity did rise sometimes, as long as it’s for less than a week I probably won’t have issues? This is imprecise and doesn’t take into account how far above 60% it was, but it’s a start to understanding.\n\n\n\nWe can also look at time below 60% RH, I’ll use a control chart to demonstrate that method. This one is an interesting case where the data is clearly skewed, however a control chart is still useful. The blue line is the average hours below 60, the red lines represent upper and lower expected limits, commonly known as 3 sigma limits. Any point outside these limits is considered potentially unusual.\n\n\nCode\nhumid_intermediate &lt;- humid_RL %&gt;% \n    group_by(RLID) %&gt;% \n    mutate(counter_var = 1) %&gt;%\n    summarize(hours_above_below_60 = sum(counter_var) * 5 / 60)\n\nhumid_RL %&gt;% \n    select(RLID, over_60) %&gt;% unique() %&gt;% \n    left_join(humid_intermediate, by = \"RLID\") %&gt;% \n#Now I have the FALSE segments aka 60 or below\n#I guess save false, and spc?\n    filter(over_60 == FALSE) %&gt;% \n    ggplot(aes(x = RLID, y = hours_above_below_60))+\n    geom_point()+\n    geom_line()+\n    stat_QC(method = \"XmR\")+\n    labs(y = \"hours below 60% RH\", \n         title = \"Control chart of run length below 60% RH\")\n\n\n\n\n\n\n\n\n\nIn the plot we see there are many short intervals below 60, some as short as 5 minutes. There are also longer intervals. The upper limit is around 100 hours(4 days), so we might say any interval above this may be unusual. Note that the lower limit is below zero, which is not a possible value. In reality this control chart is one sided, which is ok, many processes end up having a boundary.\n\n\n\n\n\nCode\nhumid_RL %&gt;% \n    select(RLID, over_60) %&gt;% unique() %&gt;% \n    left_join(humid_intermediate, by = \"RLID\") %&gt;% \n    ggplot(aes(x=RLID, y=hours_above_below_60, fill = over_60))+\n    geom_col()\n\n\n\n\n\n\n\n\n\nCode\nhumid_RL %&gt;% \n    select(RLID, over_60) %&gt;% unique() %&gt;% \n    left_join(humid_intermediate, by = \"RLID\") %&gt;% \n    mutate(over_60 = case_when(over_60 == FALSE ~ 'below 60', \n                               over_60 == TRUE  ~ 'above 60')) %&gt;% \n    ggplot(aes(x = RLID, y = hours_above_below_60))+\n    geom_point()+\n    geom_line()+\n    stat_QC(method = \"XmR\", auto.label = TRUE)+\n    facet_wrap(~over_60)+\n    labs(y = \"hours below 60% RH\", \n         title = \"Control chart comparing time above and below 60% RH\")\n\n\n\n\n\n\n\n\n\nOne last plot, we can get a sense of how the times above and below 60 compare with side by side control charts. The average for time below 60 is about 19 hours vs 7 hours for above 60. The limits for below are much wider, and there are many points outside the limit. This indicates that much more time is spent below 60%RH than above.\n\n\n\nI hope you found this exploratory analysis interesting, I may come back and try doing some forecasting later but I have already answered the questions at hand! Humidity does not seem to be an issue, at least not this summer. I plan to try forecasting radon, as that may be a difficult challenge. This was an intuitive analysis, and I wanted to demonstrate how many problems can be evaluated with control charts."
  },
  {
    "objectID": "posts/Airqualityanalysis/index.html#introduction",
    "href": "posts/Airqualityanalysis/index.html#introduction",
    "title": "Indoor Air Quality Analysis",
    "section": "",
    "text": "I’m running an Airthings Wave Plus air quality meter in my basement.\nI got it awhile ago because the price was right, and I figured it would make for some interesting data projects!\nIt logs radon every hour, and every 5 minutes it logs: carbon dioxide, VOCs, humidity, temperature, and pressure. Airthings has a great phone app (bluetooth) and excellent web dashboards. For this project, I’ll be downloading the logs and working with them in R. They have an API if you want to get real time data, I plan to run a project with that in the future, but for now we’ll work with a fixed dataset.\nRadon is important to me - I live in an area where it can be a problem. Temperature isn’t too critical but since it’s logged anyways we’ll look at it, might also be an important predictor for other measurements. Humidity in the basement is important to know, I want to make sure it doesn’t get above 60% RH, at least not for long. We don’t want to have mold issues. I don’t know that it’s necessary to measure humidity, I believe I can feel when it’s too high, but since we are logging it will be nice information to have, to confirm my sensory experience. Pressure might be used as a predictor for forecasting. VOC is important to look at, I want to have healthy air. \\(CO_2\\) is (to me) a proxy for “freshness”, I doubt it will be at a harmful level, but elevated levels might indicate a need for ventilation.\nI assume through this project that the Wave Plus is accurate for each measurement. I don’t have secondary measurements for any of these to verify against. For what it’s worth I did have an inexpensive temperature/humidity meter running at the same time for awhile, but the battery ran out and I haven’t replaced it. The readings appeared very similar to the Wave Plus.\n\n\nCode\n### read in dataset\n#it comes in a single column, separated by \";\"\n# wave_data &lt;- read_delim(\"airthings_export_110623.csv\",\n#                           delim = \";\",\n#                           escape_double = FALSE,\n#                           col_types = cols(recorded = col_character()),\n#                           trim_ws = TRUE)\n# ### need to cleanup date column, there is a \"T\" between date & time\n# ### and turn it into date format\n# wave_data &lt;- wave_data %&gt;%\n#     mutate(recorded = as_datetime(gsub(pattern = \"T\",\n#                               replacement = \" \",\n#                               x = wave_data$recorded))\n#     ) %&gt;%\n# ### rename columns for convenience\n#     rename(date_time   = recorded,\n#            radon       = `RADON_SHORT_TERM_AVG pCi/L`,\n#            temperature = `TEMP °F`,\n#            humidity    = `HUMIDITY %`,\n#            pressure    = `PRESSURE mBar`,\n#            CO2         = `CO2 ppm`,\n#            VOC         = `VOC ppb`)\n# \n# ### remove first week, calibration period\n# ### you know close enough, let's just start april 1st\n# wave_data &lt;- wave_data %&gt;%\n#     filter(date_time &gt;= \"2023-04-01 00:00:00\")\n# \n# saveRDS(wave_data, file = \"wave_data.RDS\")\n\n\n\n\nCode\n#load the processed file\nwave_data &lt;- readRDS(\"wave_data.RDS\")\n\n\n\n\nFirst I will look at relative humidity. Humidity is a potential problem in the warmer months.\n\n\nCode\np1 &lt;- wave_data %&gt;%\n    select(date_time, humidity) %&gt;%\n    ggplot(aes(x=date_time, y=humidity))+\n    #geom_point()+\n    geom_line()+\n    labs(x = \"\", y = \"\", title = \"%RH April - November, average about 59\")+\n    geom_hline(yintercept = mean(wave_data$humidity), color = \"blue\")+\n    geom_hline(yintercept = 60, color = \"red\")\n\np2 &lt;- wave_data %&gt;%\n    select(date_time, humidity) %&gt;%\n    ggplot(aes(x=humidity))+\n    geom_histogram(color = \"white\", fill = \"light blue\") +\n    theme(axis.line = element_blank(),\n                                axis.text = element_blank(),\n                                axis.ticks = element_blank(),\n                                axis.title = element_blank()) + \n    coord_flip()\n\np1 + p2 + plot_layout(widths = c(5,1))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nLooking over this period, I can see in general a lot of the readings are below 60, with some above 60, and a few rare points above 65. It looks like RH is higher in May, and October - this might line up with the fact that the air conditioning isn’t running around those times. Makes a lot of sense that way. It’s hard to see each day, let’s look at a smaller time period to check for patterns:\n\n\nCode\nwave_data[30000:32000, ] %&gt;%\n    select(date_time, humidity) %&gt;%\n    ggplot(aes(x=date_time, y=humidity))+\n    #geom_point()+\n    geom_line()\n\n\n\n\n\n\n\n\n\nI didn’t mention yet, %RH is recorded in 0.5% intervals, so there is some chunkiness. I believe measuring every 5 minutes is excessive, I could probably measure once an hour (or less) or take a daily average. Humidity rises and falls but I don’t know how predictable that’s going to be. Definitely cyclic and it looks like it rises through the day, and falls during the night. Not surprising.\nInteractive plot for exploration:\n\n\nCode\nwave_data %&gt;%\n    select(date_time, humidity) %&gt;%\n    ggplot(aes(x=date_time, y=humidity))+\n    geom_line()+\n    labs(x = \"\", y = \"\", title = \"%RH April - November, average about 59\")+\n    geom_hline(yintercept = mean(wave_data$humidity), color = \"blue\")+\n    geom_hline(yintercept = 60, color = \"red\") -&gt; p \n\n    ggplotly(p, dynamicTicks = TRUE) %&gt;% \n        rangeslider()\n\n\n\n\n\n\nZooming in on sections helps me see the cyclic nature, in general humidity does rise during the day and fall at night.\n\n\nI’ve already learned enough to answer my question!\nThe basement felt fine, no issues! No need for a dehumidifier. Unless/until something changes - next year? Never?\nIn general, %RH stayed in a good range, there were some higher days but this didn’t seem to cause problems. Next I’ll look at how long the humidity was above 60%, even though it went higher from time to time I bet it was for relatively short intervals.\nKnowing how long humidity was above 60% will help me understand my conclusion that %RH was not an issue this year.\n\n\n\n\n\n\nCode\nhumid_RL &lt;- wave_data %&gt;% \n    select(date_time, humidity) %&gt;% \n    mutate(over_60 = case_when(humidity &gt; 60 ~ TRUE, \n                               .default = FALSE))\nsetDT(humid_RL)\nhumid_RL$RLID &lt;- rleid(humid_RL$over_60) #run length for gt/lt 60\n\nhumid_RL[over_60 == TRUE] %&gt;% \n    group_by(RLID) %&gt;% \n    summarize(hours_above_60 = (sum(over_60)) * 5 / 60) %&gt;% \n\n    ggplot(aes(x=RLID, y=hours_above_60))+\n    geom_col()+\n    labs(x = \"Run ID\", y = \"hours above 60% RH\", title = \"How long was each period of time above 60%RH?\")\n\n\n\n\n\n\n\n\n\nThis is neat - I see that most excursions above 60% RH were for about a day or less, a few were between 2-4 days, and the longest was about 6 days. Seems that while the humidity did rise sometimes, as long as it’s for less than a week I probably won’t have issues? This is imprecise and doesn’t take into account how far above 60% it was, but it’s a start to understanding.\n\n\n\nWe can also look at time below 60% RH, I’ll use a control chart to demonstrate that method. This one is an interesting case where the data is clearly skewed, however a control chart is still useful. The blue line is the average hours below 60, the red lines represent upper and lower expected limits, commonly known as 3 sigma limits. Any point outside these limits is considered potentially unusual.\n\n\nCode\nhumid_intermediate &lt;- humid_RL %&gt;% \n    group_by(RLID) %&gt;% \n    mutate(counter_var = 1) %&gt;%\n    summarize(hours_above_below_60 = sum(counter_var) * 5 / 60)\n\nhumid_RL %&gt;% \n    select(RLID, over_60) %&gt;% unique() %&gt;% \n    left_join(humid_intermediate, by = \"RLID\") %&gt;% \n#Now I have the FALSE segments aka 60 or below\n#I guess save false, and spc?\n    filter(over_60 == FALSE) %&gt;% \n    ggplot(aes(x = RLID, y = hours_above_below_60))+\n    geom_point()+\n    geom_line()+\n    stat_QC(method = \"XmR\")+\n    labs(y = \"hours below 60% RH\", \n         title = \"Control chart of run length below 60% RH\")\n\n\n\n\n\n\n\n\n\nIn the plot we see there are many short intervals below 60, some as short as 5 minutes. There are also longer intervals. The upper limit is around 100 hours(4 days), so we might say any interval above this may be unusual. Note that the lower limit is below zero, which is not a possible value. In reality this control chart is one sided, which is ok, many processes end up having a boundary.\n\n\n\n\n\nCode\nhumid_RL %&gt;% \n    select(RLID, over_60) %&gt;% unique() %&gt;% \n    left_join(humid_intermediate, by = \"RLID\") %&gt;% \n    ggplot(aes(x=RLID, y=hours_above_below_60, fill = over_60))+\n    geom_col()\n\n\n\n\n\n\n\n\n\nCode\nhumid_RL %&gt;% \n    select(RLID, over_60) %&gt;% unique() %&gt;% \n    left_join(humid_intermediate, by = \"RLID\") %&gt;% \n    mutate(over_60 = case_when(over_60 == FALSE ~ 'below 60', \n                               over_60 == TRUE  ~ 'above 60')) %&gt;% \n    ggplot(aes(x = RLID, y = hours_above_below_60))+\n    geom_point()+\n    geom_line()+\n    stat_QC(method = \"XmR\", auto.label = TRUE)+\n    facet_wrap(~over_60)+\n    labs(y = \"hours below 60% RH\", \n         title = \"Control chart comparing time above and below 60% RH\")\n\n\n\n\n\n\n\n\n\nOne last plot, we can get a sense of how the times above and below 60 compare with side by side control charts. The average for time below 60 is about 19 hours vs 7 hours for above 60. The limits for below are much wider, and there are many points outside the limit. This indicates that much more time is spent below 60%RH than above.\n\n\n\nI hope you found this exploratory analysis interesting, I may come back and try doing some forecasting later but I have already answered the questions at hand! Humidity does not seem to be an issue, at least not this summer. I plan to try forecasting radon, as that may be a difficult challenge. This was an intuitive analysis, and I wanted to demonstrate how many problems can be evaluated with control charts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Join me as I go through projects that interest me."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ben’s projects",
    "section": "",
    "text": "Indoor Air Quality Analysis\n\n\n\n\n\n\nAirquality\n\n\nintuitive analysis\n\n\nSPC\n\n\n\nEvaluating air quality\n\n\n\n\n\nBen Jepson\n\n\n\n\n\n\n\n\n\n\n\n\nAustralia Production Forecasting\n\n\n\n\n\n\nintuitive analysis\n\n\nforecasting\n\n\nprediction\n\n\nSPC\n\n\n\nSPC applied to forecasting & monitoring\n\n\n\n\n\nBen Jepson\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nBen Jepson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Beerforecasting/index.html",
    "href": "posts/Beerforecasting/index.html",
    "title": "Australia Production Forecasting",
    "section": "",
    "text": "I’d like to walk through an example of using SPC (statistical process control) to understand a non-manufacturing dataset. Control charts are useful for any process that happens over time. For this project I’ll use the aus_production dataset from the fpp3 package. It contains quarterly data from 1951 - 2010 of production amounts of beer, tobacco, bricks, cement, electricity, and gas.\nRather than looking at the entire dataset, I’d like to walk through it as if this was a new process we’re tracking, and starting at the beginning. I’ll be using XmR aka ImR control charts, the chart for individual values. The value is plotted with lines marking the average, and upper/lower limits. The limits are known as 3 sigma limits, and we can use them as a clue to see if the process is changing over time and whether it is predictable. The limits are based on the differences between successive values.\n\n\nCode\n#import aus_production dataset from fpp3\naus &lt;- data.frame(aus_production)\n\n#original variable Quarter contains both year YYYY and quarter, want to split for my purposes here - probably will want to separate by quarter\n#the year YYYY: as \"year\"\naus$year = as.integer(sapply(strsplit(as.character(aus$Quarter), split = \" \"), \"[[\", 1))\n\n#the quarter Q1, Q2, Q3 etc as \"quarter\"\naus$quarter = sapply(strsplit(as.character(aus$Quarter), split = \" \"), \"[[\", 2)\n\n#original variable was called \"Quarter\" which is confusing, rename it to be more descriptive\ncolnames(aus)[colnames(aus) == 'Quarter'] &lt;- 'year_quarter'\n\n\naus %&gt;% \n    pivot_longer(cols = -c(year_quarter,year, quarter), names_to = \"product\", values_to = \"quantity\") \n\n\n# A tibble: 1,308 × 5\n   year_quarter  year quarter product     quantity\n          &lt;qtr&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n 1      1956 Q1  1956 Q1      Beer             284\n 2      1956 Q1  1956 Q1      Tobacco         5225\n 3      1956 Q1  1956 Q1      Bricks           189\n 4      1956 Q1  1956 Q1      Cement           465\n 5      1956 Q1  1956 Q1      Electricity     3923\n 6      1956 Q1  1956 Q1      Gas                5\n 7      1956 Q2  1956 Q2      Beer             213\n 8      1956 Q2  1956 Q2      Tobacco         5178\n 9      1956 Q2  1956 Q2      Bricks           204\n10      1956 Q2  1956 Q2      Cement           532\n# ℹ 1,298 more rows\n\n\n\n\nSay we have recently started tracking production of beer (in megaliters). We have just the past year’s data, recorded by quarter:\n\n\nCode\nbeer_df &lt;- select(aus, quarter, year, year_quarter, Beer)\n\nbeer_df[1:4, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 3)+ geom_line()+\n    labs(y = \"Beer (megaliters)\", title = \"First year\")\n\n\n\n\n\n\n\n\n\nGood so far. We see Q2 and Q3 are lowest, but we don’t know if this is meaningful or if this pattern will hold in future years. We could set up a control chart with just this year. Typically we would want more values before calculating limits, but we can start with as few values as we want really. Just remember these limits aren’t based on much data, and should be treated with a grain of salt, and revisited when we have more data.\n\n\nCode\nbeer_df[1:4, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 3)+ geom_line()+\n    stat_QC(method = \"XmR\", auto.label = TRUE)+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First year control chart - limits are wide\")\n\n\n\n\n\n\n\n\n\nBased on the control chart, we predict future values will fall somewhere between 110.8 and 405.2. Let’s add another year and see what happens.\nFirst, I’ll add the next year using the limits calculated on just year 1 (1956).\n\n\nCode\nbeer_df[1:8, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 3)+ geom_line()+\n    #stat_QC(method = \"XmR\", auto.label = TRUE)+\n    geom_hline(yintercept = c(405.2,258, 110.8), color = c(\"red\", \"blue\", \"red\"))+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First 2 years, limits calculated with year 1\")\n\n\n\n\n\n\n\n\n\nIt does look like there might be a quarterly pattern already. We might want to start breaking out quarters and tracking them on their own. I’d like to see if production in each quarter is growing, declining, or staying the same.\nWith year 3 I’ll start tracking by quarter in addition to all quarters together:\n\n\nCode\nbeer_df[1:12, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 3)+ geom_line()+\n    #stat_QC(method = \"XmR\", auto.label = TRUE)+\n    geom_hline(yintercept = c(405.2,258, 110.8), color = c(\"red\", \"blue\", \"red\"))+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First 3 years, limits calculated with year 1\")\n\n\n\n\n\n\n\n\n\nCode\nbeer_df[1:12, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 3)+ geom_line()+\n    stat_QC(method = \"XmR\", auto.label = TRUE)+\n    #geom_hline(yintercept = c(405.2,258, 110.8), color = c(\"red\", \"blue\", \"red\"))+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First 3 years, limits calculated with years 1-3\")\n\n\n\n\n\n\n\n\n\nRecalculating the limits with 3 years of data has narrowed them somewhat, but I want to point this out - they aren’t very different than the limits calculated with only year 1 (4 data points!). It’s ok to calculate early and revisit.\nThe quarterly differences are very clear now, let’s look at each quarter:\n\n\nCode\nbeer_df[1:12, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 2)+\n    geom_line()+\n    stat_QC(method = \"XmR\", auto.label = FALSE)+\n    facet_wrap(~quarter, ncol = 4)+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First 3 years, limits calculated by quarter, with years 1-3\")\n\n\n\n\n\n\n\n\n\nThe limits are very wide (relatively) due to the small number of data points per quarter. However, they are all much narrower than when we looked at all quarters together, indicating that the differences between quarters were inflating the variation observed.\n\n\n\nSuppose we want to see production go up over time, the chart will help us see if this is happening.\n\n\nCode\nbeer_df[1:20, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 2)+\n    geom_line()+\n    stat_QC(method = \"XmR\", auto.label = FALSE)+\n    facet_wrap(~quarter, ncol = 4)+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First 5 years, limits calculated by quarter, with years 1-5\")\n\n\n\n\n\n\n\n\n\nCode\n#I'm just entering the limits by hand, it's quicker right now than doing with\n#code, though that's easy to do with ggQC\nlimits5df &lt;- data.frame(quarter = c(\"Q1\", \"Q2\",\"Q3\",\"Q4\"),\n                        UL  = c(318, 243, 264, 329), \n                        avg = c(273, 226, 242, 313), \n                        LL  = c(228, 208, 220, 298)\n                        )\n\n\nLimits for Q1 are still relatively wider than other quarters. I don’t see an upward trend yet, maybe in Q2 and Q3? With SPC, we’re not sure yet…I have seen many patterns like this in the real world that turned out to just be noise.\n\n\n\nIf I made an animation of this, we could see it unfold point by point, I’ll work on that another time. (Right now I’m holding myself to a deadline so here’s another static plot - what do the first 10 years look like?)\nBefore we look at this, note that I’m locking the limits we calculated with years 1-5 data.\n\n\nCode\nbeer_df[1:40, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 2)+\n    geom_line()+\n    geom_hline(data = limits5df, aes(yintercept = UL), color = \"red\")+\n    geom_hline(data = limits5df, aes(yintercept = LL), color = \"red\")+\n    geom_hline(data = limits5df, aes(yintercept = avg), color = \"blue\")+\n    facet_wrap(~quarter, ncol = 4)+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First 10 years, limits calculated by quarter with years 1-5\")\n\n\n\n\n\n\n\n\n\nA few interesting things (what do you see?): Seems like definite upward trends.\nBut when might we start to conclude that?\nQ2 and Q3 both have a point above the upper limit in year 7, Q4 exceeds the upper limit in year 6. Any point outside a limit indicates something unusual. After these years, the points continue to rise, at year 7 or 8 I would believe there really is an upward trend and start tracking differently.\n\n\n\nSometimes there might be a tendency to overreact to data points, control charts are great for putting data in context. While it might look like there is an upward trend in some quarters, we got better confirmation of a change around year 6/7. Around year 8 I would switch to a trended control chart, fitting a regression line and putting limits around it to continue monitoring beer production.\n\n\n\nI’ll put together trended control charts, and discuss the interpretation and calculations used."
  },
  {
    "objectID": "posts/Beerforecasting/index.html#introduction",
    "href": "posts/Beerforecasting/index.html#introduction",
    "title": "Australia Production Forecasting",
    "section": "",
    "text": "I’d like to walk through an example of using SPC (statistical process control) to understand a non-manufacturing dataset. Control charts are useful for any process that happens over time. For this project I’ll use the aus_production dataset from the fpp3 package. It contains quarterly data from 1951 - 2010 of production amounts of beer, tobacco, bricks, cement, electricity, and gas.\nRather than looking at the entire dataset, I’d like to walk through it as if this was a new process we’re tracking, and starting at the beginning. I’ll be using XmR aka ImR control charts, the chart for individual values. The value is plotted with lines marking the average, and upper/lower limits. The limits are known as 3 sigma limits, and we can use them as a clue to see if the process is changing over time and whether it is predictable. The limits are based on the differences between successive values.\n\n\nCode\n#import aus_production dataset from fpp3\naus &lt;- data.frame(aus_production)\n\n#original variable Quarter contains both year YYYY and quarter, want to split for my purposes here - probably will want to separate by quarter\n#the year YYYY: as \"year\"\naus$year = as.integer(sapply(strsplit(as.character(aus$Quarter), split = \" \"), \"[[\", 1))\n\n#the quarter Q1, Q2, Q3 etc as \"quarter\"\naus$quarter = sapply(strsplit(as.character(aus$Quarter), split = \" \"), \"[[\", 2)\n\n#original variable was called \"Quarter\" which is confusing, rename it to be more descriptive\ncolnames(aus)[colnames(aus) == 'Quarter'] &lt;- 'year_quarter'\n\n\naus %&gt;% \n    pivot_longer(cols = -c(year_quarter,year, quarter), names_to = \"product\", values_to = \"quantity\") \n\n\n# A tibble: 1,308 × 5\n   year_quarter  year quarter product     quantity\n          &lt;qtr&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n 1      1956 Q1  1956 Q1      Beer             284\n 2      1956 Q1  1956 Q1      Tobacco         5225\n 3      1956 Q1  1956 Q1      Bricks           189\n 4      1956 Q1  1956 Q1      Cement           465\n 5      1956 Q1  1956 Q1      Electricity     3923\n 6      1956 Q1  1956 Q1      Gas                5\n 7      1956 Q2  1956 Q2      Beer             213\n 8      1956 Q2  1956 Q2      Tobacco         5178\n 9      1956 Q2  1956 Q2      Bricks           204\n10      1956 Q2  1956 Q2      Cement           532\n# ℹ 1,298 more rows\n\n\n\n\nSay we have recently started tracking production of beer (in megaliters). We have just the past year’s data, recorded by quarter:\n\n\nCode\nbeer_df &lt;- select(aus, quarter, year, year_quarter, Beer)\n\nbeer_df[1:4, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 3)+ geom_line()+\n    labs(y = \"Beer (megaliters)\", title = \"First year\")\n\n\n\n\n\n\n\n\n\nGood so far. We see Q2 and Q3 are lowest, but we don’t know if this is meaningful or if this pattern will hold in future years. We could set up a control chart with just this year. Typically we would want more values before calculating limits, but we can start with as few values as we want really. Just remember these limits aren’t based on much data, and should be treated with a grain of salt, and revisited when we have more data.\n\n\nCode\nbeer_df[1:4, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 3)+ geom_line()+\n    stat_QC(method = \"XmR\", auto.label = TRUE)+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First year control chart - limits are wide\")\n\n\n\n\n\n\n\n\n\nBased on the control chart, we predict future values will fall somewhere between 110.8 and 405.2. Let’s add another year and see what happens.\nFirst, I’ll add the next year using the limits calculated on just year 1 (1956).\n\n\nCode\nbeer_df[1:8, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 3)+ geom_line()+\n    #stat_QC(method = \"XmR\", auto.label = TRUE)+\n    geom_hline(yintercept = c(405.2,258, 110.8), color = c(\"red\", \"blue\", \"red\"))+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First 2 years, limits calculated with year 1\")\n\n\n\n\n\n\n\n\n\nIt does look like there might be a quarterly pattern already. We might want to start breaking out quarters and tracking them on their own. I’d like to see if production in each quarter is growing, declining, or staying the same.\nWith year 3 I’ll start tracking by quarter in addition to all quarters together:\n\n\nCode\nbeer_df[1:12, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 3)+ geom_line()+\n    #stat_QC(method = \"XmR\", auto.label = TRUE)+\n    geom_hline(yintercept = c(405.2,258, 110.8), color = c(\"red\", \"blue\", \"red\"))+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First 3 years, limits calculated with year 1\")\n\n\n\n\n\n\n\n\n\nCode\nbeer_df[1:12, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 3)+ geom_line()+\n    stat_QC(method = \"XmR\", auto.label = TRUE)+\n    #geom_hline(yintercept = c(405.2,258, 110.8), color = c(\"red\", \"blue\", \"red\"))+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First 3 years, limits calculated with years 1-3\")\n\n\n\n\n\n\n\n\n\nRecalculating the limits with 3 years of data has narrowed them somewhat, but I want to point this out - they aren’t very different than the limits calculated with only year 1 (4 data points!). It’s ok to calculate early and revisit.\nThe quarterly differences are very clear now, let’s look at each quarter:\n\n\nCode\nbeer_df[1:12, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 2)+\n    geom_line()+\n    stat_QC(method = \"XmR\", auto.label = FALSE)+\n    facet_wrap(~quarter, ncol = 4)+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First 3 years, limits calculated by quarter, with years 1-3\")\n\n\n\n\n\n\n\n\n\nThe limits are very wide (relatively) due to the small number of data points per quarter. However, they are all much narrower than when we looked at all quarters together, indicating that the differences between quarters were inflating the variation observed.\n\n\n\nSuppose we want to see production go up over time, the chart will help us see if this is happening.\n\n\nCode\nbeer_df[1:20, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 2)+\n    geom_line()+\n    stat_QC(method = \"XmR\", auto.label = FALSE)+\n    facet_wrap(~quarter, ncol = 4)+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First 5 years, limits calculated by quarter, with years 1-5\")\n\n\n\n\n\n\n\n\n\nCode\n#I'm just entering the limits by hand, it's quicker right now than doing with\n#code, though that's easy to do with ggQC\nlimits5df &lt;- data.frame(quarter = c(\"Q1\", \"Q2\",\"Q3\",\"Q4\"),\n                        UL  = c(318, 243, 264, 329), \n                        avg = c(273, 226, 242, 313), \n                        LL  = c(228, 208, 220, 298)\n                        )\n\n\nLimits for Q1 are still relatively wider than other quarters. I don’t see an upward trend yet, maybe in Q2 and Q3? With SPC, we’re not sure yet…I have seen many patterns like this in the real world that turned out to just be noise.\n\n\n\nIf I made an animation of this, we could see it unfold point by point, I’ll work on that another time. (Right now I’m holding myself to a deadline so here’s another static plot - what do the first 10 years look like?)\nBefore we look at this, note that I’m locking the limits we calculated with years 1-5 data.\n\n\nCode\nbeer_df[1:40, ] %&gt;% \n    ggplot(aes(x = year_quarter, y = Beer))+\n    geom_point(size = 2)+\n    geom_line()+\n    geom_hline(data = limits5df, aes(yintercept = UL), color = \"red\")+\n    geom_hline(data = limits5df, aes(yintercept = LL), color = \"red\")+\n    geom_hline(data = limits5df, aes(yintercept = avg), color = \"blue\")+\n    facet_wrap(~quarter, ncol = 4)+\n    labs(y = \"Beer (megaliters)\", \n         title = \"First 10 years, limits calculated by quarter with years 1-5\")\n\n\n\n\n\n\n\n\n\nA few interesting things (what do you see?): Seems like definite upward trends.\nBut when might we start to conclude that?\nQ2 and Q3 both have a point above the upper limit in year 7, Q4 exceeds the upper limit in year 6. Any point outside a limit indicates something unusual. After these years, the points continue to rise, at year 7 or 8 I would believe there really is an upward trend and start tracking differently.\n\n\n\nSometimes there might be a tendency to overreact to data points, control charts are great for putting data in context. While it might look like there is an upward trend in some quarters, we got better confirmation of a change around year 6/7. Around year 8 I would switch to a trended control chart, fitting a regression line and putting limits around it to continue monitoring beer production.\n\n\n\nI’ll put together trended control charts, and discuss the interpretation and calculations used."
  }
]